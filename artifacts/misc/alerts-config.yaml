# Alert Configuration for IntelliFlow CRM
# Task: IFC-117 - AI Model Monitoring
# Created: 2025-12-29

apiVersion: monitoring.intelliflow.com/v1
kind: AlertConfiguration
metadata:
  name: intelliflow-ai-alerts
  version: 1.0.0
  created: 2025-12-29

# Global alert settings
global:
  evaluation_interval: 30s
  notification_channels:
    - slack
    - pagerduty
    - email
  default_severity: warning
  repeat_interval: 4h
  
# Alert groups
groups:
  # AI Model Drift Alerts
  - name: ai-drift
    rules:
      - alert: AIModelDriftDetected
        expr: intelliflow_ai_drift_detected{} == 1
        for: 5m
        labels:
          severity: warning
          team: ai-platform
        annotations:
          summary: "Model drift detected for {{ $labels.model }}"
          description: "Model {{ $labels.model }} showing distribution drift on metric {{ $labels.metric }}"
          runbook: "docs/operations/runbooks/ai-drift.md"
          
      - alert: AIModelDriftCritical
        expr: intelliflow_ai_drift_score{severity="high"} > 0.3
        for: 10m
        labels:
          severity: critical
          team: ai-platform
        annotations:
          summary: "Critical drift score for {{ $labels.model }}"
          description: "Drift score {{ $value }} exceeds critical threshold"
          
  # AI Hallucination Alerts
  - name: ai-hallucination
    rules:
      - alert: AIHallucinationRateHigh
        expr: intelliflow_ai_hallucination_rate > 0.05
        for: 5m
        labels:
          severity: warning
          team: ai-platform
        annotations:
          summary: "Hallucination rate above 5% threshold"
          description: "Current rate: {{ $value | humanizePercentage }}"
          
      - alert: AIHallucinationRateCritical
        expr: intelliflow_ai_hallucination_rate > 0.10
        for: 5m
        labels:
          severity: critical
          team: ai-platform
        annotations:
          summary: "Hallucination rate critically high"
          description: "Current rate: {{ $value | humanizePercentage }}, KPI violated"
          
  # AI Latency Alerts
  - name: ai-latency
    rules:
      - alert: AILatencyP95Exceeded
        expr: intelliflow_ai_latency_p95_ms > 2000
        for: 5m
        labels:
          severity: warning
          team: ai-platform
        annotations:
          summary: "AI P95 latency exceeds SLO"
          description: "P95 latency: {{ $value }}ms (target: 2000ms)"
          
      - alert: AILatencyP99Exceeded
        expr: intelliflow_ai_latency_p99_ms > 5000
        for: 5m
        labels:
          severity: critical
          team: ai-platform
        annotations:
          summary: "AI P99 latency critically high"
          description: "P99 latency: {{ $value }}ms (target: 5000ms)"
          
      - alert: AISLOViolation
        expr: intelliflow_ai_slo_compliant == 0
        for: 1m
        labels:
          severity: critical
          team: ai-platform
        annotations:
          summary: "AI SLO violation detected"
          description: "AI operations failing to meet SLO requirements"
          
  # AI ROI Alerts
  - name: ai-roi
    rules:
      - alert: AIROINegative
        expr: intelliflow_ai_roi_current < 0
        for: 1h
        labels:
          severity: warning
          team: ai-platform
        annotations:
          summary: "AI ROI is negative"
          description: "Current ROI: {{ $value }}%, costs exceeding value"
          
      - alert: AIROIBelowTarget
        expr: intelliflow_ai_roi_current < 200 AND intelliflow_ai_roi_current >= 0
        for: 24h
        labels:
          severity: info
          team: ai-platform
        annotations:
          summary: "AI ROI below target"
          description: "Current ROI: {{ $value }}% (target: 200%)"
          
      - alert: AICostSpike
        expr: rate(intelliflow_ai_total_cost[1h]) > 0.5
        for: 30m
        labels:
          severity: warning
          team: ai-platform
          team_2: finance
        annotations:
          summary: "AI cost increasing rapidly"
          description: "Cost rate: ${{ $value }}/hour"
          
  # AI Operation Health
  - name: ai-health
    rules:
      - alert: AISuccessRateLow
        expr: intelliflow_ai_success_rate < 0.95
        for: 10m
        labels:
          severity: warning
          team: ai-platform
        annotations:
          summary: "AI operation success rate dropping"
          description: "Success rate: {{ $value | humanizePercentage }}"
          
      - alert: AISuccessRateCritical
        expr: intelliflow_ai_success_rate < 0.90
        for: 5m
        labels:
          severity: critical
          team: ai-platform
        annotations:
          summary: "AI operations failing at high rate"
          description: "Success rate critically low: {{ $value | humanizePercentage }}"
          
      - alert: AIModelUnhealthy
        expr: intelliflow_ai_model_health_score < 70
        for: 15m
        labels:
          severity: warning
          team: ai-platform
        annotations:
          summary: "AI model health degraded"
          description: "Health score: {{ $value }}% for {{ $labels.model }}"

# Notification routing
routes:
  - match:
      severity: critical
    receiver: pagerduty-ai
    continue: true
    
  - match:
      severity: critical
    receiver: slack-alerts
    
  - match:
      severity: warning
      team: ai-platform
    receiver: slack-ai-team
    
  - match:
      severity: info
    receiver: email-digest
    group_wait: 1h
    group_interval: 4h

# Receivers
receivers:
  - name: pagerduty-ai
    pagerduty_configs:
      - service_key: ${PAGERDUTY_AI_KEY}
        severity: "{{ .Labels.severity }}"
        
  - name: slack-alerts
    slack_configs:
      - channel: "#alerts-critical"
        send_resolved: true
        title: "{{ .Labels.alertname }}"
        text: "{{ .Annotations.description }}"
        
  - name: slack-ai-team
    slack_configs:
      - channel: "#ai-platform"
        send_resolved: true
        
  - name: email-digest
    email_configs:
      - to: ai-team@intelliflow.com
        send_resolved: false

# Inhibition rules
inhibit_rules:
  - source_match:
      severity: critical
    target_match:
      severity: warning
    equal:
      - alertname
      - model
